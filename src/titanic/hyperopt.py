import optuna
import pandas as pd
import numpy as np
import joblib
import os
import config
import datetime
from sklearn import metrics
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import cross_val_score
from sklearn import ensemble, tree
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier

# ç¦ç”¨optunaçš„æ—¥å¿—è¾“å‡º
optuna.logging.set_verbosity(optuna.logging.WARNING)

def get_model_with_params(model_name, trial):
    """æ ¹æ®æ¨¡å‹åç§°å’Œtrialè·å–å…·æœ‰ä¼˜åŒ–å‚æ•°çš„æ¨¡å‹"""
    
    if model_name == "rf":
        return ensemble.RandomForestClassifier(
            n_estimators=trial.suggest_int("n_estimators", 50, 300),
            max_depth=trial.suggest_int("max_depth", 3, 20),
            min_samples_split=trial.suggest_int("min_samples_split", 2, 20),
            min_samples_leaf=trial.suggest_int("min_samples_leaf", 1, 10),
            random_state=42
        )
    
    elif model_name == "xgb":
        return XGBClassifier(
            n_estimators=trial.suggest_int("n_estimators", 50, 300),
            max_depth=trial.suggest_int("max_depth", 3, 10),
            learning_rate=trial.suggest_float("learning_rate", 0.01, 0.3),
            subsample=trial.suggest_float("subsample", 0.6, 1.0),
            colsample_bytree=trial.suggest_float("colsample_bytree", 0.6, 1.0),
            random_state=42
        )
    
    elif model_name == "lgbm":
        return LGBMClassifier(
            n_estimators=trial.suggest_int("n_estimators", 50, 300),
            max_depth=trial.suggest_int("max_depth", 3, 15),
            learning_rate=trial.suggest_float("learning_rate", 0.01, 0.3),
            subsample=trial.suggest_float("subsample", 0.6, 1.0),
            colsample_bytree=trial.suggest_float("colsample_bytree", 0.6, 1.0),
            random_state=42,
            verbose=-1
        )
    
    elif model_name == "cat":
        return CatBoostClassifier(
            iterations=trial.suggest_int("iterations", 50, 300),
            depth=trial.suggest_int("depth", 3, 10),
            learning_rate=trial.suggest_float("learning_rate", 0.01, 0.3),
            l2_leaf_reg=trial.suggest_float("l2_leaf_reg", 1, 10),
            random_state=42,
            verbose=False
        )
    
    elif model_name == "decision_tree_gini":
        return tree.DecisionTreeClassifier(
            criterion="gini",
            max_depth=trial.suggest_int("max_depth", 3, 20),
            min_samples_split=trial.suggest_int("min_samples_split", 2, 20),
            min_samples_leaf=trial.suggest_int("min_samples_leaf", 1, 10),
            random_state=42
        )
    
    else:
        raise ValueError(f"Unsupported model: {model_name}")

def prepare_data():
    """å‡†å¤‡æ•°æ®"""
    df = pd.read_csv(config.TRAINING_FILE)
    
    # ä½¿ç”¨çš„ç‰¹å¾
    features = ["Pclass", "Sex", "SibSp", "Parch"]
    
    # å®šä¹‰é¢„å¤„ç†å™¨
    categorical_features = ["Sex"]
    numeric_features = ["Pclass", "SibSp", "Parch"]
    
    preprocessor = ColumnTransformer(
        transformers=[
            ("cat", OneHotEncoder(handle_unknown="ignore"), categorical_features),
            ("num", "passthrough", numeric_features),
        ]
    )
    
    X = df[features]
    y = df["Survived"].values
    folds = df["kfold"].values
    
    return X, y, folds, preprocessor

def objective(trial, model_name, X, y, folds, preprocessor):
    """optunaçš„ç›®æ ‡å‡½æ•°"""
    
    # è·å–æ¨¡å‹
    model = get_model_with_params(model_name, trial)
    
    # æ„å»ºpipeline
    clf = Pipeline([
        ("preprocessor", preprocessor),
        ("model", model)
    ])
    
    # æ‰‹åŠ¨è¿›è¡Œäº¤å‰éªŒè¯
    scores = []
    for fold in range(5):
        # åˆ†å‰²æ•°æ®
        train_idx = folds != fold
        val_idx = folds == fold
        
        X_train, X_val = X[train_idx], X[val_idx]
        y_train, y_val = y[train_idx], y[val_idx]
        
        # è®­ç»ƒå’Œé¢„æµ‹
        clf.fit(X_train, y_train)
        y_pred = clf.predict(X_val)
        
        # è®¡ç®—å‡†ç¡®ç‡
        score = metrics.accuracy_score(y_val, y_pred)
        scores.append(score)
    
    return np.mean(scores)

def optimize_hyperparameters(model_name, n_trials=100):
    """ä¼˜åŒ–è¶…å‚æ•°"""
    
    print(f"ğŸš€ å¼€å§‹ä¼˜åŒ– {model_name} çš„è¶…å‚æ•°...")
    
    # å‡†å¤‡æ•°æ®
    X, y, folds, preprocessor = prepare_data()
    
    # åˆ›å»ºstudy
    study = optuna.create_study(direction='maximize')
    
    # ä¼˜åŒ–
    study.optimize(
        lambda trial: objective(trial, model_name, X, y, folds, preprocessor),
        n_trials=n_trials,
        show_progress_bar=True
    )
    
    # è¾“å‡ºç»“æœ
    print(f"âœ… {model_name} ä¼˜åŒ–å®Œæˆ!")
    print(f"ğŸ“Š æœ€ä½³å¾—åˆ†: {study.best_value:.4f}")
    print(f"ğŸ¯ æœ€ä½³å‚æ•°: {study.best_params}")
    
    # ä¿å­˜ç»“æœ
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    results = {
        'model_name': model_name,
        'best_score': study.best_value,
        'best_params': study.best_params,
        'timestamp': timestamp
    }
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(config.MODEL_OUTPUT, exist_ok=True)
    
    # ä¿å­˜æœ€ä½³å‚æ•°
    joblib.dump(results, os.path.join(config.MODEL_OUTPUT, f"{model_name}_best_params_{timestamp}.joblib"))
    
    return study.best_params, study.best_value

def train_best_model(model_name, best_params):
    """ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæ‰€æœ‰foldçš„æ¨¡å‹"""
    
    print(f"ğŸ‹ï¸ ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒ {model_name} æ¨¡å‹...")
    
    # å‡†å¤‡æ•°æ®
    X, y, folds, preprocessor = prepare_data()
    
    results = []
    
    for fold in range(5):
        print(f"  è®­ç»ƒ Fold {fold}...")
        
        # åˆ†å‰²æ•°æ®
        train_idx = folds != fold
        val_idx = folds == fold
        
        X_train, X_val = X[train_idx], X[val_idx]
        y_train, y_val = y[train_idx], y[val_idx]
        
        # åˆ›å»ºæ¨¡å‹
        if model_name == "rf":
            model = ensemble.RandomForestClassifier(**best_params, random_state=42)
        elif model_name == "xgb":
            model = XGBClassifier(**best_params, random_state=42)
        elif model_name == "lgbm":
            model = LGBMClassifier(**best_params, random_state=42, verbose=-1)
        elif model_name == "cat":
            model = CatBoostClassifier(**best_params, random_state=42, verbose=False)
        elif model_name == "decision_tree_gini":
            model = tree.DecisionTreeClassifier(**best_params, criterion="gini", random_state=42)
        
        # æ„å»ºpipeline
        clf = Pipeline([
            ("preprocessor", preprocessor),
            ("model", model)
        ])
        
        # è®­ç»ƒ
        clf.fit(X_train, y_train)
        
        # é¢„æµ‹
        y_pred = clf.predict(X_val)
        accuracy = metrics.accuracy_score(y_val, y_pred)
        
        print(f"    Fold {fold} å‡†ç¡®ç‡: {accuracy:.4f}")
        results.append(accuracy)
        
        # ä¿å­˜æ¨¡å‹
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{model_name}_optimized_{timestamp}_fold{fold}.joblib"
        joblib.dump(clf, os.path.join(config.MODEL_OUTPUT, filename))
    
    print(f"ğŸ“ˆ å¹³å‡å‡†ç¡®ç‡: {np.mean(results):.4f} Â± {np.std(results):.4f}")
    
    return results

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="è¶…å‚æ•°ä¼˜åŒ–")
    parser.add_argument("--model", type=str, required=True, 
                       choices=["rf", "xgb", "lgbm", "cat", "decision_tree_gini"],
                       help="è¦ä¼˜åŒ–çš„æ¨¡å‹")
    parser.add_argument("--trials", type=int, default=100, 
                       help="ä¼˜åŒ–è¯•éªŒæ¬¡æ•°")
    parser.add_argument("--train", action="store_true",
                       help="æ˜¯å¦ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæ¨¡å‹")
    
    args = parser.parse_args()
    
    print(f"ğŸ” æ­£åœ¨ä¼˜åŒ–æ¨¡å‹: {args.model}")
    print(f"ğŸ² è¯•éªŒæ¬¡æ•°: {args.trials}")
    print("=" * 50)
    
    # ä¼˜åŒ–è¶…å‚æ•°
    best_params, best_score = optimize_hyperparameters(args.model, args.trials)
    
    if args.train:
        print("\n" + "=" * 50)
        # ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæ¨¡å‹
        train_best_model(args.model, best_params)
    
    print("\nğŸ‰ ä¼˜åŒ–å®Œæˆ!") 