import optuna
import pandas as pd
import numpy as np
import joblib
import os
import config
import datetime
import preprocessing
from sklearn import metrics
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import cross_val_score
from sklearn import ensemble, tree

# å®‰å…¨å¯¼å…¥å¯é€‰ä¾èµ–
def safe_import(module_name, class_name):
    """å®‰å…¨å¯¼å…¥å¯é€‰ä¾èµ–"""
    try:
        module = __import__(module_name, fromlist=[class_name])
        return getattr(module, class_name)
    except ImportError:
        print(f"âš ï¸ è­¦å‘Š: {module_name} æœªå®‰è£…ï¼Œ{class_name} æ¨¡å‹ä¸å¯ç”¨")
        return None

XGBClassifier = safe_import('xgboost', 'XGBClassifier')
LGBMClassifier = safe_import('lightgbm', 'LGBMClassifier') 
CatBoostClassifier = safe_import('catboost', 'CatBoostClassifier')

# ç¦ç”¨optunaçš„æ—¥å¿—è¾“å‡º
optuna.logging.set_verbosity(optuna.logging.WARNING)

def get_model_with_params(model_name, trial):
    """æ ¹æ®æ¨¡å‹åç§°å’Œtrialè·å–å…·æœ‰ä¼˜åŒ–å‚æ•°çš„æ¨¡å‹"""
    
    if model_name == "rf":
        return ensemble.RandomForestClassifier(
            n_estimators=trial.suggest_int("n_estimators", 50, 300),
            max_depth=trial.suggest_int("max_depth", 3, 20),
            min_samples_split=trial.suggest_int("min_samples_split", 2, 20),
            min_samples_leaf=trial.suggest_int("min_samples_leaf", 1, 10),
            random_state=42
        )
    
    elif model_name == "xgb":
        if XGBClassifier is None:
            raise ValueError("XGBoostæœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨xgbæ¨¡å‹")
        return XGBClassifier(
            n_estimators=trial.suggest_int("n_estimators", 50, 300),
            max_depth=trial.suggest_int("max_depth", 3, 10),
            learning_rate=trial.suggest_float("learning_rate", 0.01, 0.3),
            subsample=trial.suggest_float("subsample", 0.6, 1.0),
            colsample_bytree=trial.suggest_float("colsample_bytree", 0.6, 1.0),
            random_state=42
        )
    
    elif model_name == "lgbm":
        if LGBMClassifier is None:
            raise ValueError("LightGBMæœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨lgbmæ¨¡å‹")
        return LGBMClassifier(
            n_estimators=trial.suggest_int("n_estimators", 50, 300),
            max_depth=trial.suggest_int("max_depth", 3, 15),
            learning_rate=trial.suggest_float("learning_rate", 0.01, 0.3),
            subsample=trial.suggest_float("subsample", 0.6, 1.0),
            colsample_bytree=trial.suggest_float("colsample_bytree", 0.6, 1.0),
            random_state=42,
            verbose=-1
        )
    
    elif model_name == "cat":
        if CatBoostClassifier is None:
            raise ValueError("CatBoostæœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨catæ¨¡å‹")
        return CatBoostClassifier(
            iterations=trial.suggest_int("iterations", 50, 300),
            depth=trial.suggest_int("depth", 3, 10),
            learning_rate=trial.suggest_float("learning_rate", 0.01, 0.3),
            l2_leaf_reg=trial.suggest_float("l2_leaf_reg", 1, 10),
            random_state=42,
            verbose=False
        )
    
    elif model_name == "decision_tree_gini":
        return tree.DecisionTreeClassifier(
            criterion="gini",
            max_depth=trial.suggest_int("max_depth", 3, 20),
            min_samples_split=trial.suggest_int("min_samples_split", 2, 20),
            min_samples_leaf=trial.suggest_int("min_samples_leaf", 1, 10),
            random_state=42
        )
    
    else:
        raise ValueError(f"Unsupported model: {model_name}")

def prepare_data(feature_config='recommended'):
    """å‡†å¤‡æ•°æ®"""
    print(f"ğŸ”§ å‡†å¤‡æ•°æ®ï¼Œç‰¹å¾é…ç½®: {feature_config}")
    
    # è¯»å–è®­ç»ƒæ•°æ®ï¼ˆå¸¦foldä¿¡æ¯ï¼‰
    df = pd.read_csv(config.TRAINING_FILE)
    
    # æ ¹æ®ç‰¹å¾é…ç½®é€‰æ‹©æ•°æ®å¤„ç†æ–¹å¼
    if feature_config == 'baseline':
        # ä½¿ç”¨åŸå§‹baselineç‰¹å¾
        features = ["Pclass", "Sex", "SibSp", "Parch"]
        
        # å®šä¹‰é¢„å¤„ç†å™¨
        categorical_features = ["Sex"]
        numeric_features = ["Pclass", "SibSp", "Parch"]
        
        preprocessor = ColumnTransformer(
            transformers=[
                ("cat", OneHotEncoder(handle_unknown="ignore"), categorical_features),
                ("num", "passthrough", numeric_features),
            ]
        )
        
        X = df[features]
        
    else:
        # ä½¿ç”¨å·¥ç¨‹åŒ–ç‰¹å¾
        if 'TitleGroup' not in df.columns:
            print("ğŸ“Š æ£€æµ‹åˆ°åŸå§‹æ•°æ®ï¼Œå¼€å§‹åº”ç”¨ç‰¹å¾å·¥ç¨‹...")
            # è¯»å–åŸå§‹è®­ç»ƒæ•°æ®è¿›è¡Œç‰¹å¾å·¥ç¨‹
            train_raw = pd.read_csv("../../input/titanic/train.csv")
            
            # åº”ç”¨ç‰¹å¾å·¥ç¨‹
            preprocessor_fe = preprocessing.TitanicPreprocessor()
            train_engineered = preprocessor_fe.fit_transform(train_raw)
            
            # å°†kfoldä¿¡æ¯åˆå¹¶å›å»
            train_engineered['kfold'] = df['kfold']
            df = train_engineered
        
        # è·å–é¢„å¤„ç†pipelineå’Œç‰¹å¾åˆ—è¡¨
        preprocessor, features = preprocessing.create_preprocessing_pipeline(feature_config)
        X = df[features]
    
    y = df["Survived"].values
    folds = df["kfold"].values
    
    print(f"ğŸ“ ä½¿ç”¨ç‰¹å¾: {features}")
    print(f"ğŸ“Š æ•°æ®å½¢çŠ¶: {X.shape}")
    
    return X, y, folds, preprocessor, features

def objective(trial, model_name, X, y, folds, preprocessor):
    """optunaçš„ç›®æ ‡å‡½æ•°"""
    
    # è·å–æ¨¡å‹
    model = get_model_with_params(model_name, trial)
    
    # æ„å»ºpipeline
    clf = Pipeline([
        ("preprocessor", preprocessor),
        ("model", model)
    ])
    
    # æ‰‹åŠ¨è¿›è¡Œäº¤å‰éªŒè¯
    scores = []
    for fold in range(5):
        # åˆ†å‰²æ•°æ®
        train_idx = folds != fold
        val_idx = folds == fold
        
        X_train, X_val = X[train_idx], X[val_idx]
        y_train, y_val = y[train_idx], y[val_idx]
        
        # è®­ç»ƒå’Œé¢„æµ‹
        clf.fit(X_train, y_train)
        y_pred = clf.predict(X_val)
        
        # è®¡ç®—å‡†ç¡®ç‡
        score = metrics.accuracy_score(y_val, y_pred)
        scores.append(score)
    
    return np.mean(scores)

def optimize_hyperparameters(model_name, n_trials=100, feature_config='recommended'):
    """ä¼˜åŒ–è¶…å‚æ•°"""
    
    print(f"ğŸš€ å¼€å§‹ä¼˜åŒ– {model_name} çš„è¶…å‚æ•°...")
    print(f"ğŸ¯ ç‰¹å¾é…ç½®: {feature_config}")
    
    # å‡†å¤‡æ•°æ®
    X, y, folds, preprocessor, features = prepare_data(feature_config)
    
    # åˆ›å»ºstudy
    study = optuna.create_study(direction='maximize')
    
    # ä¼˜åŒ–
    study.optimize(
        lambda trial: objective(trial, model_name, X, y, folds, preprocessor),
        n_trials=n_trials,
        show_progress_bar=True
    )
    
    # è¾“å‡ºç»“æœ
    print(f"âœ… {model_name} ä¼˜åŒ–å®Œæˆ!")
    print(f"ğŸ“Š æœ€ä½³å¾—åˆ†: {study.best_value:.4f}")
    print(f"ğŸ¯ æœ€ä½³å‚æ•°: {study.best_params}")
    
    # ä¿å­˜ç»“æœ
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    results = {
        'model_name': model_name,
        'feature_config': feature_config,
        'best_score': study.best_value,
        'best_params': study.best_params,
        'features_used': features,
        'timestamp': timestamp
    }
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(config.MODEL_OUTPUT, exist_ok=True)
    
    # ä¿å­˜æœ€ä½³å‚æ•°
    joblib.dump(results, os.path.join(config.MODEL_OUTPUT, f"{model_name}_best_params_{feature_config}_{timestamp}.joblib"))
    
    return study.best_params, study.best_value

def train_best_model(model_name, best_params, feature_config='recommended'):
    """ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæ‰€æœ‰foldçš„æ¨¡å‹"""
    
    print(f"ğŸ‹ï¸ ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒ {model_name} æ¨¡å‹...")
    print(f"ğŸ¯ ç‰¹å¾é…ç½®: {feature_config}")
    
    # å‡†å¤‡æ•°æ®
    X, y, folds, preprocessor, features = prepare_data(feature_config)
    
    results = []
    
    for fold in range(5):
        print(f"  è®­ç»ƒ Fold {fold}...")
        
        # åˆ†å‰²æ•°æ®
        train_idx = folds != fold
        val_idx = folds == fold
        
        X_train, X_val = X[train_idx], X[val_idx]
        y_train, y_val = y[train_idx], y[val_idx]
        
        # åˆ›å»ºæ¨¡å‹
        if model_name == "rf":
            model = ensemble.RandomForestClassifier(**best_params, random_state=42)
        elif model_name == "xgb":
            if XGBClassifier is None:
                raise ValueError("XGBoostæœªå®‰è£…")
            model = XGBClassifier(**best_params, random_state=42)
        elif model_name == "lgbm":
            if LGBMClassifier is None:
                raise ValueError("LightGBMæœªå®‰è£…")
            model = LGBMClassifier(**best_params, random_state=42, verbose=-1)
        elif model_name == "cat":
            if CatBoostClassifier is None:
                raise ValueError("CatBoostæœªå®‰è£…")
            model = CatBoostClassifier(**best_params, random_state=42, verbose=False)
        elif model_name == "decision_tree_gini":
            model = tree.DecisionTreeClassifier(**best_params, criterion="gini", random_state=42)
        
        # æ„å»ºpipeline
        clf = Pipeline([
            ("preprocessor", preprocessor),
            ("model", model)
        ])
        
        # è®­ç»ƒ
        clf.fit(X_train, y_train)
        
        # é¢„æµ‹
        y_pred = clf.predict(X_val)
        accuracy = metrics.accuracy_score(y_val, y_pred)
        
        print(f"    Fold {fold} å‡†ç¡®ç‡: {accuracy:.4f}")
        results.append(accuracy)
        
        # ä¿å­˜æ¨¡å‹
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{model_name}_optimized_{feature_config}_{timestamp}_fold{fold}.joblib"
        joblib.dump(clf, os.path.join(config.MODEL_OUTPUT, filename))
    
    print(f"ğŸ“ˆ å¹³å‡å‡†ç¡®ç‡: {np.mean(results):.4f} Â± {np.std(results):.4f}")
    
    return results

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="è¶…å‚æ•°ä¼˜åŒ–")
    parser.add_argument("--model", type=str, required=True, 
                       choices=["rf", "xgb", "lgbm", "cat", "decision_tree_gini"],
                       help="è¦ä¼˜åŒ–çš„æ¨¡å‹")
    parser.add_argument("--trials", type=int, default=100, 
                       help="ä¼˜åŒ–è¯•éªŒæ¬¡æ•°")
    parser.add_argument("--train", action="store_true",
                       help="æ˜¯å¦ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæ¨¡å‹")
    parser.add_argument("--features", type=str, default="baseline",
                       choices=["baseline", "core", "recommended", "all"],
                       help="ç‰¹å¾é…ç½®: baseline(åŸå§‹4ç‰¹å¾), core(æ ¸å¿ƒ3ç‰¹å¾), recommended(æ¨è8ç‰¹å¾), all(å…¨éƒ¨ç‰¹å¾)")
    
    args = parser.parse_args()
    
    print(f"ğŸ” æ­£åœ¨ä¼˜åŒ–æ¨¡å‹: {args.model}")
    print(f"ğŸ² è¯•éªŒæ¬¡æ•°: {args.trials}")
    print(f"ğŸ“ ç‰¹å¾é…ç½®: {args.features}")
    print("=" * 50)
    
    # ä¼˜åŒ–è¶…å‚æ•°
    best_params, best_score = optimize_hyperparameters(args.model, args.trials, args.features)
    
    if args.train:
        print("\n" + "=" * 50)
        # ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæ¨¡å‹
        train_best_model(args.model, best_params, args.features)
    
    print("\nğŸ‰ ä¼˜åŒ–å®Œæˆ!") 