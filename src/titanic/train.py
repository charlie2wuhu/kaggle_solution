import joblib
import pandas as pd
import os
import glob
import config
import argparse
import model_dispatcher
from sklearn import metrics
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
import datetime

def load_best_params(model_name):
    """åŠ è½½æŒ‡å®šæ¨¡å‹çš„æœ€æ–°æœ€ä½³å‚æ•°"""
    # æŸ¥æ‰¾è¯¥æ¨¡å‹çš„æ‰€æœ‰æœ€ä½³å‚æ•°æ–‡ä»¶
    pattern = os.path.join(config.MODEL_OUTPUT, f"{model_name}_best_params_*.joblib")
    files = glob.glob(pattern)
    
    if not files:
        print(f"âš ï¸ æ²¡æœ‰æ‰¾åˆ°æ¨¡å‹ {model_name} çš„æœ€ä½³å‚æ•°æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°")
        return None
    
    # æŒ‰æ—¶é—´æˆ³æ’åºï¼Œè·å–æœ€æ–°çš„æ–‡ä»¶
    latest_file = max(files, key=os.path.getctime)
    
    print(f"ğŸ“ åŠ è½½æœ€ä½³å‚æ•°æ–‡ä»¶: {latest_file}")
    
    # åŠ è½½å‚æ•°
    results = joblib.load(latest_file)
    
    print(f"ğŸ¯ æœ€ä½³å¾—åˆ†: {results['best_score']:.4f}")
    print(f"ğŸ“Š æœ€ä½³å‚æ•°: {results['best_params']}")
    print(f"â° æ—¶é—´æˆ³: {results['timestamp']}")
    
    return results['best_params']

def create_model_with_best_params(model_name, best_params):
    """æ ¹æ®æ¨¡å‹åç§°å’Œæœ€ä½³å‚æ•°åˆ›å»ºæ¨¡å‹"""
    if best_params is None:
        # å¦‚æœæ²¡æœ‰æœ€ä½³å‚æ•°ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹
        return model_dispatcher.models[model_name]
    
    if model_name == "rf":
        from sklearn import ensemble
        return ensemble.RandomForestClassifier(**best_params, random_state=42)
    elif model_name == "xgb":
        from xgboost import XGBClassifier
        return XGBClassifier(**best_params, random_state=42)
    elif model_name == "lgbm":
        from lightgbm import LGBMClassifier
        return LGBMClassifier(**best_params, random_state=42, verbose=-1)
    elif model_name == "cat":
        from catboost import CatBoostClassifier
        return CatBoostClassifier(**best_params, random_state=42, verbose=False)
    elif model_name in ["decision_tree_gini", "decision_tree_entropy"]:
        from sklearn import tree
        criterion = "gini" if model_name == "decision_tree_gini" else "entropy"
        return tree.DecisionTreeClassifier(**best_params, criterion=criterion, random_state=42)
    else:
        # å¦‚æœæ¨¡å‹ä¸æ”¯æŒå‚æ•°ä¼˜åŒ–ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹
        return model_dispatcher.models[model_name]

def run(fold, model):
    # è¯»å–æ•°æ®æ–‡ä»¶
    df = pd.read_csv(config.TRAINING_FILE)
    # é€‰å–dfä¸­kfoldåˆ—ä¸ç­‰äºfold
    df_train = df[df.kfold != fold].reset_index(drop=True)
    # é€‰å–dfä¸­kfoldåˆ—ç­‰äºfold
    df_valid = df[df.kfold == fold].reset_index(drop=True)
    # è®­ç»ƒé›†è¾“å…¥ï¼Œåˆ é™¤labelåˆ—

    # åªä½¿ç”¨éƒ¨åˆ†ç‰¹å¾è¿›è¡Œè®­ç»ƒå’Œé¢„æµ‹
    features = ["Pclass", "Sex", "SibSp", "Parch"]

    # å®šä¹‰é¢„å¤„ç†ï¼Œå°†Sexå­—æ®µè¿›è¡Œç‹¬çƒ­ç¼–ç 
    categorical_features = ["Sex"]
    numeric_features = ["Pclass", "SibSp", "Parch"]

    use_model = model_dispatcher.models[model]

    preprocessor = ColumnTransformer(
        transformers=[
            ("cat", OneHotEncoder(handle_unknown="ignore"), categorical_features),
            ("num", "passthrough", numeric_features),
        ]
    )

    # æ„å»ºpipeline
    clf = Pipeline(
        steps=[
            ("preprocessor", preprocessor),
            ("model", use_model)
        ]
    )

    x_train = df_train[features]
    y_train = df_train.Survived.values
    x_valid = df_valid[features]
    y_valid = df_valid.Survived.values

    print(f"Training {use_model} for fold {fold}")
    # ä½¿ç”¨pipelineè®­ç»ƒ
    clf.fit(x_train, y_train)
    # ä½¿ç”¨éªŒè¯é›†è¾“å…¥å¾—åˆ°é¢„æµ‹ç»“æœ
    preds = clf.predict(x_valid)
    # è®¡ç®—éªŒè¯é›†å‡†ç¡®ç‡
    accuracy = metrics.accuracy_score(y_valid, preds)
    # æ‰“å°foldä¿¡æ¯å’Œå‡†ç¡®ç‡
    print(f"Fold={fold}, Accuracy={accuracy}")
    # ä¿å­˜æ¨¡å‹
    # è·å–å½“å‰æ—¶é—´ï¼Œæ ¼å¼ä¸ºYYYYMMDD_HHMMSS
    now = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    # è·å–ç®—æ³•åç§°
    algo = model
    # è·å–å‚æ•°æ‘˜è¦ï¼ˆè¿™é‡Œåªåšç®€å•æ‘˜è¦ï¼Œå¯ä»¥æ ¹æ®éœ€è¦è‡ªå®šä¹‰ï¼‰
    param_summary = ""
    if hasattr(use_model, "get_params"):
        params = use_model.get_params()
        # å–éƒ¨åˆ†å‚æ•°åšæ‘˜è¦ï¼Œæ¯”å¦‚n_estimatorsã€max_depthç­‰
        keys = [k for k in ["n_estimators", "max_depth", "criterion"] if k in params]
        param_summary = "_".join([f"{k}{params[k]}" for k in keys])
    # æ‹¼æ¥æ–‡ä»¶å
    filename = f"{algo}_{param_summary}_{now}_{fold}.joblib"
    joblib.dump(clf, os.path.join(config.MODEL_OUTPUT, filename))

def train_and_predict(model_name):
    """ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒå®Œæ•´æ•°æ®é›†å¹¶è¿›è¡Œé¢„æµ‹"""
    print(f"ğŸš€ å¼€å§‹ä½¿ç”¨ {model_name} æ¨¡å‹è¿›è¡Œè®­ç»ƒå’Œé¢„æµ‹...")
    
    # åŠ è½½æœ€ä½³å‚æ•°
    best_params = load_best_params(model_name)
    
    # è¯»å–è®­ç»ƒæ•°æ®
    train_df = pd.read_csv(config.TRAINING_FILE)
    
    # åªä½¿ç”¨éƒ¨åˆ†ç‰¹å¾è¿›è¡Œè®­ç»ƒå’Œé¢„æµ‹
    features = ["Pclass", "Sex", "SibSp", "Parch"]
    
    # å®šä¹‰é¢„å¤„ç†ï¼Œå°†Sexå­—æ®µè¿›è¡Œç‹¬çƒ­ç¼–ç 
    categorical_features = ["Sex"]
    numeric_features = ["Pclass", "SibSp", "Parch"]
    
    preprocessor = ColumnTransformer(
        transformers=[
            ("cat", OneHotEncoder(handle_unknown="ignore"), categorical_features),
            ("num", "passthrough", numeric_features),
        ]
    )
    
    # åˆ›å»ºä½¿ç”¨æœ€ä½³å‚æ•°çš„æ¨¡å‹
    use_model = create_model_with_best_params(model_name, best_params)
    
    # æ„å»ºpipeline
    clf = Pipeline(
        steps=[
            ("preprocessor", preprocessor),
            ("model", use_model)
        ]
    )
    
    # å‡†å¤‡è®­ç»ƒæ•°æ®
    X_train = train_df[features]
    y_train = train_df["Survived"].values
    
    print(f"ğŸ‹ï¸ ä½¿ç”¨å®Œæ•´æ•°æ®é›†è®­ç»ƒæ¨¡å‹...")
    print(f"ğŸ“Š è®­ç»ƒæ ·æœ¬æ•°é‡: {len(X_train)}")
    
    # ä½¿ç”¨å®Œæ•´æ•°æ®é›†è®­ç»ƒ
    clf.fit(X_train, y_train)
    
    # è¯»å–æµ‹è¯•æ•°æ®
    test_df = pd.read_csv(config.TEST_FILE)
    X_test = test_df[features]
    
    print(f"ğŸ”® å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹...")
    print(f"ğŸ“Š æµ‹è¯•æ ·æœ¬æ•°é‡: {len(X_test)}")
    
    # è¿›è¡Œé¢„æµ‹
    predictions = clf.predict(X_test)
    
    # åˆ›å»ºæäº¤æ–‡ä»¶
    submission = pd.DataFrame({
        'PassengerId': test_df['PassengerId'],
        'Survived': predictions
    })
    
    # ç¡®ä¿outputç›®å½•å­˜åœ¨
    os.makedirs("output", exist_ok=True)
    
    # ä¿å­˜é¢„æµ‹ç»“æœ
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"submission_{model_name}_{timestamp}.csv"
    output_file = os.path.join(config.OUTPUT_FILE, filename)
    submission.to_csv(output_file, index=False)
    
    print(f"ğŸ’¾ é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    print(f"ğŸ“ˆ é¢„æµ‹æ ·æœ¬æ•°é‡: {len(submission)}")
    print(f"ğŸ¯ å­˜æ´»é¢„æµ‹æ•°é‡: {predictions.sum()}")
    print(f"ğŸ’€ æ­»äº¡é¢„æµ‹æ•°é‡: {len(predictions) - predictions.sum()}")
    
    # ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹
    filename = f"{model_name}_final_model_{timestamp}.joblib"
    joblib.dump(clf, os.path.join(config.MODEL_OUTPUT, filename))
    print(f"ğŸ—ƒï¸ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜åˆ°: {filename}")
    return submission, clf

if __name__ == "__main__":
    # å®ä¾‹åŒ–å‚æ•°ç¯å¢ƒ
    parser = argparse.ArgumentParser()
    # foldå‚æ•°
    parser.add_argument("--fold", type=int, help="äº¤å‰éªŒè¯foldç¼–å·")
    # æ¨¡å‹å‚æ•°
    parser.add_argument("--model", type=str, required=True, help="è¦ä½¿ç”¨çš„æ¨¡å‹")
    # æ–°å¢é¢„æµ‹æ¨¡å¼å‚æ•°
    parser.add_argument("--predict", action="store_true", help="ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒå®Œæ•´æ•°æ®é›†å¹¶é¢„æµ‹")
    # è¯»å–å‚æ•°
    args = parser.parse_args()
    
    if args.predict:
        # é¢„æµ‹æ¨¡å¼ï¼šä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒå®Œæ•´æ•°æ®é›†å¹¶é¢„æµ‹
        train_and_predict(args.model)
    else:
        # ä¼ ç»Ÿäº¤å‰éªŒè¯æ¨¡å¼
        if args.fold is None:
            print("âŒ äº¤å‰éªŒè¯æ¨¡å¼éœ€è¦æŒ‡å®š --fold å‚æ•°")
            exit(1)
        run(fold=args.fold, model=args.model)