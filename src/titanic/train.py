import joblib
import pandas as pd
import os
import glob
import config
import argparse
import model_dispatcher
import preprocessing
import datetime
from sklearn import metrics, ensemble
from sklearn import tree
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier

def load_best_params(model_name, feature_config='baseline'):
    """åŠ è½½æŒ‡å®šæ¨¡å‹çš„æœ€æ–°æœ€ä½³å‚æ•°"""

    if feature_config == 'baseline':
        return None
    
    # ä¼˜å…ˆæŸ¥æ‰¾æŒ‡å®šç‰¹å¾é…ç½®çš„å‚æ•°æ–‡ä»¶
    pattern = os.path.join(config.MODEL_OUTPUT, f"{model_name}_best_params_{feature_config}_*.joblib")
    files = glob.glob(pattern)
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æŒ‡å®šé…ç½®çš„æ–‡ä»¶ï¼ŒæŸ¥æ‰¾æ‰€æœ‰è¯¥æ¨¡å‹çš„å‚æ•°æ–‡ä»¶
    if not files:
        pattern = os.path.join(config.MODEL_OUTPUT, f"{model_name}_best_params_*.joblib")
        files = glob.glob(pattern)
    
    if not files:
        print(f"âš ï¸ æ²¡æœ‰æ‰¾åˆ°æ¨¡å‹ {model_name} çš„æœ€ä½³å‚æ•°æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°")
        return None
    
    # æŒ‰æ—¶é—´æˆ³æ’åºï¼Œè·å–æœ€æ–°çš„æ–‡ä»¶
    latest_file = max(files, key=os.path.getctime)
    
    print(f"ğŸ“ åŠ è½½æœ€ä½³å‚æ•°æ–‡ä»¶: {latest_file}")
    
    # åŠ è½½å‚æ•°
    results = joblib.load(latest_file)
    print(f"ğŸ“Š æœ€ä½³å‚æ•°: {results['best_params']}")
    return results['best_params']

def create_model_with_best_params(model_name, best_params):
    """æ ¹æ®æ¨¡å‹åç§°å’Œæœ€ä½³å‚æ•°åˆ›å»ºæ¨¡å‹"""
    if best_params is None:
        # å¦‚æœæ²¡æœ‰æœ€ä½³å‚æ•°ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹
        return model_dispatcher.models[model_name]
    
    if model_name == "rf":
        return ensemble.RandomForestClassifier(**best_params, random_state=42)
    elif model_name == "xgb":
        return XGBClassifier(**best_params, random_state=42)
    elif model_name == "lgbm":
        return LGBMClassifier(**best_params, random_state=42, verbose=-1)
    elif model_name == "cat":
        return CatBoostClassifier(**best_params, random_state=42, verbose=False)
    elif model_name in ["decision_tree_gini", "decision_tree_entropy"]:
        criterion = "gini" if model_name == "decision_tree_gini" else "entropy"
        return tree.DecisionTreeClassifier(**best_params, criterion=criterion, random_state=42)
    else:
        # å¦‚æœæ¨¡å‹ä¸æ”¯æŒå‚æ•°ä¼˜åŒ–ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹
        return model_dispatcher.models[model_name]

def run_cv_fold(fold, model_name, feature_config='baseline'):
    """æ‰§è¡Œäº¤å‰éªŒè¯ä¸­çš„å•ä¸ªfold"""

    best_params = load_best_params(model_name, feature_config)
    
    if feature_config not in config.FEATURE_CONFIG:
        raise ValueError(f"âŒ æœªçŸ¥çš„ç‰¹å¾é…ç½®: {feature_config}")
    
    features = config.FEATURE_CONFIG[feature_config]
    print(f"ğŸ“ ä½¿ç”¨ç‰¹å¾: {features}")
    df = pd.read_csv(config.TRAINING_FILE)
    
    if feature_config == 'baseline':
        # baselineæ¨¡å¼ï¼šä¸ä½¿ç”¨ç‰¹å¾å·¥ç¨‹ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹æ•°æ®
        print("âš™ï¸ ä½¿ç”¨baselineæ¨¡å¼ï¼Œä¸è¿›è¡Œç‰¹å¾å·¥ç¨‹")
        df_train = df[df.kfold != fold].reset_index(drop=True)
        df_valid = df[df.kfold == fold].reset_index(drop=True)
        
        use_model = create_model_with_best_params(model_name, best_params)
        clf, _ = preprocessing.create_full_pipeline(use_model, feature_config)
        
    else:
        train_raw = pd.read_csv("../../input/titanic/train.csv")
        preprocessor_fe = preprocessing.TitanicPreprocessor()
        train_engineered = preprocessor_fe.fit_transform(train_raw)
        
        # å°†kfoldä¿¡æ¯åˆå¹¶å›å»
        train_engineered['kfold'] = df['kfold']
        df = train_engineered
        
        # åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›†
        df_train = df[df.kfold != fold].reset_index(drop=True)
        df_valid = df[df.kfold == fold].reset_index(drop=True)
        
        use_model = create_model_with_best_params(model_name, best_params)
        clf, _ = preprocessing.create_full_pipeline(use_model, feature_config)
        clf = use_model
    
    # å‡†å¤‡æ•°æ®
    x_train = df_train[features]
    y_train = df_train['Survived'].values
    x_valid = df_valid[features]
    y_valid = df_valid['Survived'].values
    
    print(f"ğŸ‹ï¸ è®­ç»ƒæ ·æœ¬æ•°: {len(x_train)}")
    print(f"ğŸ§ª éªŒè¯æ ·æœ¬æ•°: {len(x_valid)}")
    
    # è®­ç»ƒæ¨¡å‹
    clf.fit(x_train, y_train)
    
    # éªŒè¯é›†é¢„æµ‹
    preds = clf.predict(x_valid)
    accuracy = metrics.accuracy_score(y_valid, preds)
    
    print(f"âœ… Fold={fold}, Accuracy={accuracy:.4f}")
    return accuracy

def train_full_dataset(model_name, feature_config='baseline'):
    """ä½¿ç”¨å®Œæ•´æ•°æ®é›†è®­ç»ƒå¹¶è¿›è¡Œé¢„æµ‹"""
    print(f"ğŸš€ å¼€å§‹ä½¿ç”¨ {model_name} æ¨¡å‹è¿›è¡Œè®­ç»ƒå’Œé¢„æµ‹...")
    print(f"ğŸ¯ ç‰¹å¾é…ç½®: {feature_config}")
    
    # åŠ è½½æœ€ä½³å‚æ•°
    best_params = load_best_params(model_name, feature_config)
    
    # è·å–ç‰¹å¾åˆ—è¡¨
    if feature_config not in config.FEATURE_CONFIG:
        raise ValueError(f"âŒ æœªçŸ¥çš„ç‰¹å¾é…ç½®: {feature_config}")
    
    features = config.FEATURE_CONFIG[feature_config]
    print(f"ğŸ“ ä½¿ç”¨ç‰¹å¾: {features}")
    
    if feature_config == 'baseline':
        # baselineæ¨¡å¼ï¼šä¸ä½¿ç”¨ç‰¹å¾å·¥ç¨‹
        print("âš™ï¸ ä½¿ç”¨baselineæ¨¡å¼ï¼Œä¸è¿›è¡Œç‰¹å¾å·¥ç¨‹")
        train_df = pd.read_csv(config.TRAINING_FILE)
        test_df = pd.read_csv(config.TEST_FILE)
        
        # åˆ›å»ºä½¿ç”¨æœ€ä½³å‚æ•°çš„æ¨¡å‹
        use_model = create_model_with_best_params(model_name, best_params)
        
        # è·å–é¢„å¤„ç†pipeline
        clf, _ = preprocessing.create_full_pipeline(use_model, feature_config)
        
        # å‡†å¤‡æ•°æ®
        X_train = train_df[features]
        y_train = train_df["Survived"].values
        X_test = test_df[features]
        
        # ç”¨äºåˆ›å»ºæäº¤æ–‡ä»¶çš„ID
        test_ids = test_df['PassengerId']
        
    else:
        # å…¶ä»–æ¨¡å¼ï¼šä½¿ç”¨ç‰¹å¾å·¥ç¨‹
        print("ğŸ”§ ä½¿ç”¨ç‰¹å¾å·¥ç¨‹æ¨¡å¼")
        
        # è¯»å–åŸå§‹æ•°æ®
        train_raw = pd.read_csv("../../input/titanic/train.csv")
        test_raw = pd.read_csv("../../input/titanic/test.csv")
        
        print("ğŸ”§ å¼€å§‹å¯¹è®­ç»ƒæ•°æ®åº”ç”¨ç‰¹å¾å·¥ç¨‹...")
        preprocessor_fe = preprocessing.TitanicPreprocessor()
        train_engineered = preprocessor_fe.fit_transform(train_raw)
        
        print("ğŸ”§ å¼€å§‹å¯¹æµ‹è¯•æ•°æ®åº”ç”¨ç‰¹å¾å·¥ç¨‹...")
        test_engineered = preprocessor_fe.transform(test_raw, is_training=False)
        
        # åˆ›å»ºä½¿ç”¨æœ€ä½³å‚æ•°çš„æ¨¡å‹
        use_model = create_model_with_best_params(model_name, best_params)
        
        # ç›´æ¥ä½¿ç”¨æ¨¡å‹ï¼Œå› ä¸ºç‰¹å¾å·¥ç¨‹å·²ç»å®Œæˆ
        clf = use_model
        
        # å‡†å¤‡æ•°æ®
        X_train = train_engineered[features]
        y_train = train_engineered["Survived"].values
        X_test = test_engineered[features]
        
        # ç”¨äºåˆ›å»ºæäº¤æ–‡ä»¶çš„ID
        test_ids = test_raw['PassengerId']
    
    print(f"ğŸ‹ï¸ ä½¿ç”¨å®Œæ•´æ•°æ®é›†è®­ç»ƒæ¨¡å‹...")
    print(f"ğŸ“Š è®­ç»ƒæ ·æœ¬æ•°é‡: {len(X_train)}")
    
    # ä½¿ç”¨å®Œæ•´æ•°æ®é›†è®­ç»ƒ
    clf.fit(X_train, y_train)
    
    print(f"ğŸ”® å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹...")
    print(f"ğŸ“Š æµ‹è¯•æ ·æœ¬æ•°é‡: {len(X_test)}")
    
    # è¿›è¡Œé¢„æµ‹
    predictions = clf.predict(X_test)
    
    # åˆ›å»ºæäº¤æ–‡ä»¶
    submission = pd.DataFrame({
        'PassengerId': test_ids,
        'Survived': predictions
    })
    
    # ç¡®ä¿outputç›®å½•å­˜åœ¨
    os.makedirs("../../output/titanic", exist_ok=True)
    
    # ä¿å­˜é¢„æµ‹ç»“æœ
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"submission_{model_name}_{feature_config}_{timestamp}.csv"
    output_file = os.path.join("../../output/titanic", filename)
    submission.to_csv(output_file, index=False)
    
    print(f"ğŸ’¾ é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    print(f"ğŸ“ˆ é¢„æµ‹æ ·æœ¬æ•°é‡: {len(submission)}")
    print(f"ğŸ¯ å­˜æ´»é¢„æµ‹æ•°é‡: {predictions.sum()}")
    print(f"ğŸ’€ æ­»äº¡é¢„æµ‹æ•°é‡: {len(predictions) - predictions.sum()}")
    print(f"ğŸ“Š å­˜æ´»ç‡é¢„æµ‹: {predictions.mean():.1%}")
    
    # ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹
    model_filename = f"{model_name}_final_{feature_config}_{timestamp}.joblib"
    joblib.dump(clf, os.path.join(config.MODEL_OUTPUT, model_filename))
    print(f"ğŸ—ƒï¸ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜åˆ°: {model_filename}")
    
    return submission, clf

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--model", type=str, required=True, help="è¦ä½¿ç”¨çš„æ¨¡å‹")
    parser.add_argument("--cv", type=int, required=True, choices=[0, 1], 
                       help="è®­ç»ƒæ¨¡å¼: 1=äº¤å‰éªŒè¯, 0=å®Œæ•´æ•°æ®é›†è®­ç»ƒå¹¶é¢„æµ‹")
    parser.add_argument("--fold", type=int, help="äº¤å‰éªŒè¯foldç¼–å· (cv=1æ—¶å¿…éœ€)")
    parser.add_argument("--features", type=str, default="baseline", 
                       help="ç‰¹å¾é…ç½®: baseline(åŸå§‹ç‰¹å¾ï¼Œä¸ä½¿ç”¨ç‰¹å¾å·¥ç¨‹)")
    args = parser.parse_args()
    
    print(f"ğŸ¯ æ¨¡å‹: {args.model}")
    print(f"ğŸ›ï¸ è®­ç»ƒæ¨¡å¼: {'äº¤å‰éªŒè¯' if args.cv == 1 else 'å®Œæ•´æ•°æ®é›†è®­ç»ƒ'}")
    print(f"ğŸ“ ç‰¹å¾é…ç½®: {args.features}")
    
    # éªŒè¯å‚æ•°
    if args.cv == 1 and args.fold is None:
        print("âŒ äº¤å‰éªŒè¯æ¨¡å¼ (--cv 1) éœ€è¦æŒ‡å®š --fold å‚æ•°")
        exit(1)
    
    # æ‰§è¡Œç›¸åº”çš„è®­ç»ƒæ¨¡å¼
    if args.cv == 1:
        # äº¤å‰éªŒè¯æ¨¡å¼
        print(f"ğŸ“Š æ‰§è¡Œäº¤å‰éªŒè¯ï¼Œfold: {args.fold}")
        accuracy = run_cv_fold(fold=args.fold, model_name=args.model, feature_config=args.features)
        print(f"ğŸ¯ æœ¬æ¬¡éªŒè¯å‡†ç¡®ç‡: {accuracy:.4f}")
    else:
        # å®Œæ•´æ•°æ®é›†è®­ç»ƒå¹¶é¢„æµ‹
        print("ğŸ† ä½¿ç”¨å®Œæ•´æ•°æ®é›†è®­ç»ƒå¹¶ç”Ÿæˆé¢„æµ‹")
        submission, model = train_full_dataset(model_name=args.model, feature_config=args.features)
        print("âœ… è®­ç»ƒå’Œé¢„æµ‹å®Œæˆï¼")